{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results Review Report\n",
    "\n",
    "This is supposed to help create the milestone report for now and eventuall generate it automatically. It is an interactive Jupyter document (or a static view of such a document) containing Julia code segments and their output. It is part of the JDP project which aims to create an *easily accessible* system for exploring test results and automating *arbitrary* workflows. Notebooks such as these are intended to provide an easy starting point for engineers and other technical users to create their own reports, possibly just by tweaking the existing ones.\n",
    "\n",
    "Eventually this workbook should be easily installable locally or accessible from some remote location with zero knowledge of Julia or Jupyter. However right now it is not, but you can still try by visiting: https://github.com/richiejp/jdp.\n",
    "\n",
    "Obviously you can also access the library from a REPL or use it in a traditional script or application, but Jupyter provides a nice, persistent, graphical environment. I won't discuss how to use Jupyter in this notebook (just click on help at the top), but will heavily annotate the code.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First we need to load the JDP library which does the heavy lifting; importing and transforming the test result data from OpenQA into something useable. Note that this assumes you started this notebook by running `julia src/notebook.jl`.\n",
    "\n",
    "> NOTE: It is required that you run this cell before the code cells following it. However not all of the cells need to be executed in order.\n",
    "\n",
    "You may see a bunch of horrible angry red text when running this. Unfortunately this could either be info messages or error messages from the logging system, Jupyter treats both to a red background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the init script which will setup the JDP project if necessary\n",
    "include(\"../src/init.jl\")\n",
    "\n",
    "# Bring DataFrame's _members_ into our namespace, so we can call them directly\n",
    "using DataFrames\n",
    "import Markdown: MD, @md_str\n",
    "\n",
    "# Import some libraries from the JDP project\n",
    "using JDP.OpenQA    # Contains functions for dealing with the OpenQA web API\n",
    "using JDP.TableDB   # Functions for accessing test data in table like formats (currently DataFrames)\n",
    "using JDP.Bugzilla  # Functions for accessing the Bugzilla API(s)\n",
    "using JDP.Conf      # Configuration from conf/*.toml files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set some variables which are used in later code cells. Cache type can be set to `:json` or `:binary` (which are [symbols](https://docs.julialang.org/en/v1/manual/metaprogramming/#Symbols-1)). We always save data as JSON first, but afterwards it can be copied into a binary format as well to increase loading speed.\n",
    "\n",
    "> NOTE: Julia has a _very_ strong type system, but we can still assign variables like a dynamic language. For library code it is generally a good idea to explicitly state what types you are expecting, but in Notebook code we can just let the compiler guess the type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = Conf.data(:datadir) # The cache dir for the OpenQA test result data, defaults to ~/data\n",
    "cache_type = :json # Set to :json to use the raw JSON data from OpenQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we may download some new results for a given build or builds to our local cache. This usually takes a long time, hence why there is a local cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over an array of strings... yeah I bet you really needed to be told that didn't you?\n",
    "for jid in [\"453\", \"454\"]\n",
    "    # Get some job results from the openqa.suse.de (osd) OpenQA instance.\n",
    "    # Optional arguments (after the ';') like 'build' and 'groupid' are passed to the OpenQA API\n",
    "    OpenQA.save_job_results_json(OpenQA.osd, datadir; build=\"0$jid\", groupid=\"155\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data into memory, this can also take a while. If we don't have any new data to load we can use the binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure these variables are defined in the global scope by setting them to 'nothing' which is actually\n",
    "# a value rather than really being nothing, but I shan't quibble\n",
    "json = nothing\n",
    "df = nothing\n",
    "\n",
    "if cache_type == :binary\n",
    "    # The raw data from OpenQA is absurdly huge, so to save on start up time, we can use a binary format\n",
    "    df = TableDB.load_module_results(joinpath(datadir, \"cache.jld2\"))\n",
    "else\n",
    "    json = OpenQA.load_job_results_json(datadir) # This is the raw OpenQA data in the form of a Dict\n",
    "    df = TableDB.get_module_results(json)        # This is a more refined form of the data as a DataFrame\n",
    "end\n",
    "\n",
    "\"Loaded $(nrow(df)) results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have some new data then we can update the binary cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TableDB.save_module_results(joinpath(datadir, \"cache.jld2\"), df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `describe` from the DataFrames package gives us some stats and information about the structure of the loaded data. For the raw json value we can just use `summary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(df, stats = [:nunique, :min, :max, :eltype])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the pretty table! We can also display graphs which could be even more delightful. Unfortunately it is slightly less pretty if you are viewing this as a static page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed tests for build\n",
    "\n",
    "Let's look at what tests failed for a given build. First we need to filter out passed test results and results from other builds. Then we can group the results by test name and suit, amalgamating some of the columns to make the table easier to view. Filter is fairly simple, but the grouping is a bit more complex and there is a bit of Julia magic, see [Split-Apply-Combine](http://juliadata.github.io/DataFrames.jl/stable/man/split_apply_combine.html) for help.\n",
    "\n",
    "> NOTE: Packages such as QUERY.jl allow one to use an SQL like syntax which is probably a lot easier to understand for most people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build = \"0454\"\n",
    "\n",
    "# The syntax \"var -> expr\" is an anonymous function, strings starting with 'r' are regexs.\n",
    "# In Julia you don't need to write 'return' (unless you want to return early), most \n",
    "# statements return whatever the value of the final expression is\n",
    "fails = filter(r -> r.build == build && occursin(r\"failed\", r[:result]), df)\n",
    "\n",
    "# group by name then apply the function defined by `do r ...` to each group\n",
    "# Putting `do r` after `by` is like writing `by(r -> ...`. i.e. `do r` defines a function\n",
    "# and passes it as the first argument to `by`.\n",
    "fails_by_name = by(fails, [:name, :suit]) do r\n",
    "    # 'by' first groups the results by name and suit then passes each group to us in the variable 'r'\n",
    "    # we then use 'r' to produce a new DataFrame containing a single row. We return the new DataFrames \n",
    "    # and `by` then combines them... at least I think that is what happpens.\n",
    "    DataFrame(\n",
    "        # We have to write [] otherwise DataFrame creates a multi-row result (because r.result is an array)\n",
    "        result = [unique(r.result)],\n",
    "        arch = [unique(r.arch)],\n",
    "        # Three dots `...` 'splats' an array (or tuple) into multiple function arguments \n",
    "        # and `vcat` concatenates it's arguments together\n",
    "        bugrefs = [filter(br -> br.tracker.api == nothing || br.tracker.api.name != \"OpenQA\", \n",
    "                          vcat(r.bugrefs...)) |> unique]\n",
    "        # also, and don't panic if this is a little more difficult to understand, \n",
    "        # 'unique' removes duplicate elements from a collection\n",
    "    )\n",
    "end\n",
    "\n",
    "\"$(nrow(fails_by_name)) tests failed this build\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withenv(\"LINES\" => 10) do\n",
    "    display(fails_by_name)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably have too many failures to display in Jupyter, so let's just try displaying failures for a subset of tests. We can focus on a particular test suit and remove tests which already appear to be tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missing_bugrefs = filter(fails_by_name) do r\n",
    "    length(r.bugrefs) < 1 && # Remove tests which already have bug refs\n",
    "    r.suit[1] == \"LTP\" &&    # Only include LTP results\n",
    "    r.name != \"boot_ltp\" &&  # Don't include boot_ltp and shutdown_ltp modules\n",
    "    r.name != \"shutdown_ltp\"\n",
    "end\n",
    "\n",
    "length(missing_bugrefs.name) > 0 ? missing_bugrefs : md\"Nothing to see here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_bugrefs = filter(fails_by_name) do r\n",
    "    length(r.bugrefs) < 1 &&\n",
    "    r.suit[1] == \"fstests\"\n",
    "end\n",
    "\n",
    "length(missing_bugrefs.name) > 0 ? missing_bugrefs : md\"Nothing to see here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find if any of these tests had bug refs in past builds. Note that the result of this depends on which of the above cells you ran last. This is because we reuse `missing_bugrefs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_index = Set(missing_bugrefs.name) # Convert the name column into a hash set\n",
    "\n",
    "past_results = by(filter(r -> r.name in names_index && length(r.bugrefs) > 0, df), [:name, :suit]) do r\n",
    "    DataFrame(bugrefs = [filter(unique(vcat(r.bugrefs...))) do br\n",
    "                    br.tracker.api == nothing || br.tracker.api.name != \"OpenQA\"\n",
    "                end])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now show the unique Bug references in a form you can click on.\n",
    "\n",
    "> NOTE: DataFrame's `show` implementation for HTML deliberately calls `show` with the text/plain MIME type for its cell values. So we can not make the bug refs clickable while they are in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique(vcat(past_results.bugrefs...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally show tests for which we can not find any bug references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setdiff(names_index, past_results.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
