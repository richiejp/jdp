{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Test Results Review Report\n",
    "\n",
    "This is supposed to help create the milestone QA acceptance report. It is an interactive Jupyter document (or a static view created by [Weave](https://github.com/mpastell/Weave.jl/)) containing Julia code segments and their output. It is part of the JDP project which aims to create an *easily accessible* system for exploring test results and automating *arbitrary* workflows. Notebooks such as these are intended to provide an easy starting point for engineers and other technical users to create their own reports, possibly just by tweaking the existing ones.\n",
    "\n",
    "If you are viewing the static output of this report and want to modify it then visit [gitlab.suse.de/richiejp/jdp](https://gitlab.suse.de/richiejp/jdp) for instructions on how to install and run JDP.\n",
    "\n",
    "Obviously you can also access the library from a REPL or use it in a traditional script or application, but Jupyter provides a nice, persistent, graphical environment. I won't discuss how to use Jupyter in this notebook (just click on help at the top), but will heavily annotate the code.\n",
    "\n",
    "This version of the report uses `DataFrames` to represent the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting more help\n",
    "\n",
    "Julia is self documenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@doc Int\n",
    "# hint: try @doc @doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the built in docs is often better than asking the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "First we need to load the JDP library which does the heavy lifting; importing and transforming the test result data from OpenQA into something useable. Note that this assumes you started this notebook by running `julia src/notebook.jl`.\n",
    "\n",
    "> NOTE: It is required that you run this cell before the code cells following it. However not all of the cells need to be executed in order.\n",
    "\n",
    "You may see a bunch of horrible angry red text when running this. Unfortunately this could either be info messages or error messages from the logging system, Jupyter treats both to a red background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Monitors library source files and recompiles them after most changes\n",
    "import Revise\n",
    "\n",
    "# Run the init script which will setup the JDP project if necessary\n",
    "include(\"../src/init.jl\")\n",
    "\n",
    "# Bring DataFrame's _members_ into our namespace, so we can call them directly\n",
    "using DataFrames\n",
    "using Markdown\n",
    "\n",
    "# Import some libraries from the JDP project\n",
    "using JDP.Conf\n",
    "using JDP.Trackers.OpenQA    # Contains functions for dealing with the OpenQA web API\n",
    "using JDP.Trackers.Bugzilla  # Functions for accessing the Bugzilla API(s)\n",
    "using JDP.Repository\n",
    "\n",
    "# Weave.jl can pass arguments to this notebook to control its output\n",
    "args = try\n",
    "    WEAVE_ARGS\n",
    "catch\n",
    "    Dict()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builds = get(args, \"builds\", [\"189.1\", \"0039\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "> NOTE: Julia has a _very_ strong type system, but we can still assign variables like a dynamic language. For library code it is generally a good idea to explicitly state what types you are expecting, but in Notebook code we can just let the compiler guess the type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Next we may download some new results for a given build or builds to our local cache. This usually takes a long time, hence why there is a local cache. The results are then loaded into a `DataFrame` object. Alternatively you can specify `Vector` as the second argument, in which case you will get an array of `TestResult` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Get some job results from the openqa.suse.de (osd) OpenQA instance.\n",
    "df = Repository.fetch(OpenQA.TestResult, DataFrame, \"osd\", OpenQA.RecentOrInterestingJobsDef)\n",
    "md\"There are now $(size(df, 1)) test results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The function `describe` from the DataFrames package gives us some stats and information about the structure of the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "describe(df, stats = [:nunique, :min, :max, :eltype])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Look at the pretty table! We can also display graphs which could be even more delightful. Unfortunately it is slightly less pretty if you are viewing this as a static page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Failed tests for build\n",
    "\n",
    "Let's look at what tests failed for the given builds. First we need to filter out passed test results and results from other builds. Then we can group the results by test name and suit, amalgamating some of the columns to make the table easier to view. Filter is fairly simple, but the grouping is a bit more complex and there is a bit of Julia magic, see [Split-Apply-Combine](http://juliadata.github.io/DataFrames.jl/stable/man/split_apply_combine.html) for help.\n",
    "\n",
    "> NOTE: Packages such as QUERY.jl allow one to use an SQL like syntax which is probably a lot easier to understand for most people. However I think I actually prefer using plain structs with map-filter-reduce and for-loops than either of these (See `notebooks/Propagate Bug Tags.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# The syntax \"var -> expr\" is an anonymous function, strings starting with 'r' are regexs.\n",
    "# In Julia you don't need to write 'return' (unless you want to return early), most \n",
    "# statements return whatever the value of the final expression is\n",
    "fails = filter(r -> r.build in builds && occursin(r\"failed\", r[:result]), df)\n",
    "\n",
    "# group by name then apply the function defined by `do r ...` to each group\n",
    "# Putting `do r` after `by` is like writing `by(r -> ...`. i.e. `do r` defines a function\n",
    "# and passes it as the first argument to `by`.\n",
    "fails_by_name = by(fails, [:name, :suit, :flags]) do r\n",
    "    # 'by' first groups the results by name and suit then passes each group to us in the variable 'r'\n",
    "    # we then use 'r' to produce a new DataFrame containing a single row. We return the new DataFrames \n",
    "    # and `by` then combines them... at least I think that is what happpens.\n",
    "    DataFrame(\n",
    "        # We have to write [] otherwise DataFrame creates a multi-row result (because r.result is an array)\n",
    "        result = [unique(r.result)],\n",
    "        arch = [unique(r.arch)],\n",
    "        # Three dots `...` 'splats' an array (or tuple) into multiple function arguments \n",
    "        # and `vcat` concatenates it's arguments together\n",
    "        refs = [filter(br -> br.tracker.api == nothing || br.tracker.api.name != \"OpenQA\", \n",
    "                          vcat(r.refs...)) |> unique]\n",
    "        # also, and don't panic if this is a little more difficult to understand, \n",
    "        # 'unique' removes duplicate elements from a collection\n",
    "    )\n",
    "end\n",
    "\n",
    "md\"\"\"$(nrow(fails_by_name)) tests failed in $(join(builds, \", \", \" and \"))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdisplay(data) = withenv(\"LINES\" => 100) do\n",
    "    display(data)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Even with the `bigdisplay` function above, we may have too many failures to display all at once, so let's try displaying failures for subsets of tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LTP failures\n",
    "\n",
    "Missing bug refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "no_bugrefs_ltp = filter(fails_by_name) do r\n",
    "    isempty(r.refs) && # Remove tests which already have bug refs\n",
    "    r.suit[1] == \"LTP\" &&    # Only include LTP results\n",
    "    r.name != \"boot_ltp\" &&  # Don't include boot_ltp and shutdown_ltp modules\n",
    "    r.name != \"shutdown_ltp\" &&\n",
    "    !(\"Public Cloud\" in r.flags) # LTP tests on Public cloud are displayed separately\n",
    "end\n",
    "\n",
    "bigdisplay(no_bugrefs_ltp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With bug refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "has_bugrefs_ltp = filter(fails_by_name) do r\n",
    "    !isempty(r.refs) && \n",
    "    r.suit[1] == \"LTP\" &&    # Only include LTP results\n",
    "    r.name != \"boot_ltp\" &&  # Don't include boot_ltp and shutdown_ltp modules\n",
    "    r.name != \"shutdown_ltp\" &&\n",
    "    !(\"Public Cloud\" in r.flags)\n",
    "end\n",
    "\n",
    "bigdisplay(has_bugrefs_ltp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FS test failures\n",
    "\n",
    "Missing bug refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "no_bugrefs_fs = filter(fails_by_name) do r\n",
    "    length(r.refs) < 1 &&\n",
    "    r.suit[1] == \"fstests\"\n",
    "end\n",
    "\n",
    "bigdisplay(no_bugrefs_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With bug refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_bugrefs_fs = filter(fails_by_name) do r\n",
    "    !isempty(r.refs) && r.suit[1] == \"fstests\"\n",
    "end\n",
    "\n",
    "bigdisplay(has_bugrefs_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public cloud failures\n",
    "\n",
    "Missing bug refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_bugrefs_pc = filter(fails_by_name) do r\n",
    "    isempty(r.refs) && \"Public Cloud\" in r.flags\n",
    "end\n",
    "\n",
    "bigdisplay(no_bugrefs_pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With bug refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_bugrefs_pc = filter(fails_by_name) do r\n",
    "    !isempty(r.refs) && \"Public Cloud\" in r.flags\n",
    "end\n",
    "\n",
    "bigdisplay(has_bugrefs_pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test status differences\n",
    "\n",
    "This shows a history of tests which have changed status recently.\n",
    "\n",
    "!!! note This needs updating to automatically figure out the most recent builds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the builds we want to include in the diff\n",
    "buildsv = sort([\"190.3\", \"205.7\", \"211.2\", \"212.1\"]) \n",
    "builds = Set(buildsv)\n",
    "ignore = Set([\"enable_kdump\", \"install\", \"partition\", \"run\", \"generate_report\",\n",
    "        \"boot_to_desktop\", \"boot_ltp\", \"shutdown_ltp\", \"install_ltp\", \"proc_sys_dump\"])\n",
    "\n",
    "receant = filter(df) do r\n",
    "    r.build in builds &&\n",
    "    !(r.name in ignore)\n",
    "end\n",
    "\n",
    "build_labels = [Symbol(build) for build in sort(buildsv)]\n",
    "with_builds = by(receant, [:name, :suit, :machine, :flags]) do group\n",
    "    # Remove duplicates; we can only have one result per build\n",
    "    group = by(group, :build) do sgroup\n",
    "        # If we have multiple results for one build then we take the most positive\n",
    "        # because often a test will fail due to some unrelated error and we rerun the job\n",
    "        DataFrame(\n",
    "            result = if any(r -> r == \"passed\", sgroup.result)\n",
    "                \"passed\"\n",
    "            elseif any(r -> r == \"failed\", sgroup.result)\n",
    "                \"failed\"\n",
    "            else\n",
    "                sgroup.result[1]\n",
    "            end\n",
    "        )\n",
    "    end\n",
    "    sort!(group, :build)\n",
    "    \n",
    "    results = String[]\n",
    "    for i in 1:length(buildsv)\n",
    "        # Fill in missing build results\n",
    "        if i > length(group.build) || buildsv[i] != group.build[i]\n",
    "            push!(results, \"none\")\n",
    "        else\n",
    "            push!(results, group.result[i])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # return a table with a single row of results for each build\n",
    "    DataFrame([[r] for r in results], build_labels)\n",
    "end\n",
    "\n",
    "# Remove tests where every build had the same result\n",
    "build_diff = filter(with_builds) do r\n",
    "    any(b -> b != r[build_labels[1]], (r[blabel] for blabel in build_labels[2:end]))\n",
    "end\n",
    "\n",
    "md\"$(size(build_diff, 1)) of $(size(with_builds, 1)) tests had differing results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter(build_diff) do r\n",
    "    r.suit[1] == \"LTP\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filter(build_diff) do r\n",
    "    r.suit == [\"fstests\", \"xfs\"]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filter(build_diff) do r\n",
    "    r.suit == [\"fstests\", \"btrfs\"]\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  },
  "name": "report.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
