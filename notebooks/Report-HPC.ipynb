{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC Test Report\n",
    "\n",
    "This is a *prototype* for a HPC test results report. It is part of the [JDP](https://rpalethorpe.io.suse.de/jdp/) project. This report is created using Jupyter and Julia, for details see the (highly annotated) general [kernel group report](https://rpalethorpe.io.suse.de/jdp/reports/Report-DataFrames.html).\n",
    "\n",
    "So far I have just included a test results matrix and no information about which test failures have been tagged with a bug or what that bug is. Nor have I included any test error messages or links to the failing module in OpenQA. In theory any information can be added although some things will require changes to the JDP library which will take time.\n",
    "\n",
    "Note that once the report reaches a mature state it can be converted to a form where the inline code is hidden or minimised.\n",
    "\n",
    "## Index\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "2. [Results](#Results)\n",
    "\n",
    "## Setup\n",
    "\n",
    "First we need to build up our data structures to create the test matrix. There are some stats here which may be useful, but otherwise you can safely skip this part most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitors library source files and recompiles them after most changes\n",
    "import Revise\n",
    "\n",
    "# Run the init script which will setup the JDP project if necessary\n",
    "include(\"../src/init.jl\")\n",
    "\n",
    "# Bring DataFrame's _members_ into our namespace, so we can call them directly\n",
    "using DataFrames\n",
    "\n",
    "# import the markdown string literal/macro\n",
    "import Markdown: @md_str\n",
    "\n",
    "# Import some libraries from the JDP project\n",
    "using JDP.Conf\n",
    "using JDP.Trackers.OpenQA    # Contains functions for dealing with the OpenQA web API\n",
    "using JDP.Trackers.Bugzilla  # Functions for accessing the Bugzilla API(s)\n",
    "using JDP.Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allres = Repository.fetch(OpenQA.TestResult, Vector, \"osd\", OpenQA.RecentOrInterestingJobsDef)\n",
    "\n",
    "md\"We have **$(length(allres))** results in total\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allhpcres = filter(allres) do res\n",
    "    get(res.suit, 2, nothing) == \"HPC\"\n",
    "end\n",
    "\n",
    "md\"We have **$(length(allhpcres))** HPC test module results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only show results for a single product, which can be set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"sle-15-SP1-Installer-DVD\"\n",
    "\n",
    "hpcres = filter(allhpcres) do res\n",
    "    res.product == product\n",
    "end\n",
    "\n",
    "md\"We have **$(length(hpcres))** HPC test results for $product\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builds = map(res -> res.build, hpcres) |> unique\n",
    "builds = map(b -> (parse(Float64, b), b), builds)\n",
    "sort!(builds, by=(b -> b[1]))\n",
    "\n",
    "totalbuilds = length(builds)\n",
    "recentnum = max(0, min(4, totalbuilds - 4))\n",
    "recentbuilds = join(map(b -> \"**$(b[2])**\", builds[end-recentnum+1:end]), \", \", \" and \")\n",
    "\n",
    "md\"We have **$totalbuilds** builds in total. The last $recentnum are $recentbuilds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testnames = map(res -> (res.suit[3], res.name, res.arch), hpcres) |> unique |> sort\n",
    "\n",
    "# First build a dictionary with build names for keys and tests for values\n",
    "buildsres = Dict{String, Dict{Tuple{String, String, String}, Union{Nothing, OpenQA.TestResult}}}(\n",
    "    build[2] => Dict(name => nothing for name in testnames) for \n",
    "        build in builds[end-recentnum:end]\n",
    ")\n",
    "\n",
    "# Find the best result for each build-test pair\n",
    "for res in Iterators.filter(res -> haskey(buildsres, res.build), hpcres)\n",
    "    name = (res.suit[3], res.name, res.arch)\n",
    "    bres = buildsres[res.build]\n",
    "    if bres[name] == nothing || bres[name].result != \"passed\"\n",
    "        bres[name] = res\n",
    "    end \n",
    "end\n",
    "\n",
    "# Remove builds where many of the tests were not run as this usually means there was\n",
    "# an obvious problem with the testing infrastructure\n",
    "for build in keys(buildsres)\n",
    "    none_count = 0\n",
    "    \n",
    "    for res in values(buildsres[build])\n",
    "        if res == nothing || res.result == \"none\"\n",
    "            none_count += 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if none_count / length(testnames) > 0.25\n",
    "        delete!(buildsres, build)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Remove tests which have been passing\n",
    "failed_testnames = []\n",
    "for name in testnames\n",
    "    boring = true\n",
    "    \n",
    "    for build in keys(buildsres)\n",
    "        res = buildsres[build][name] \n",
    "        if res == nothing || res.result â‰  \"passed\"\n",
    "            boring = false\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if boring\n",
    "        for build in keys(buildsres)\n",
    "            delete!(buildsres[build], name)\n",
    "        end\n",
    "    else\n",
    "        push!(failed_testnames, name)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Put the results into columns for display in a table\n",
    "buildcols = [[let res = buildsres[build][name]\n",
    "    res == nothing ? \"none\" : res.result\n",
    "end for name in failed_testnames] for build in keys(buildsres)]\n",
    "headers = [Symbol(\"Job name\"), Symbol(\"Module name\"), Symbol(\"Arch\"), Symbol.(keys(buildsres))...]\n",
    "\n",
    "md\"\"\"\n",
    "Ignoring **$(length(testnames) - length(failed_testnames))** of **$(length(testnames))** tests\n",
    "because they only had pass results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Below is a matrix of the HPC results. Test scenarious which always pass have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "withenv(\"LINES\" => 200) do\n",
    "    display(\n",
    "        DataFrame([\n",
    "            map(t -> t[1], failed_testnames), \n",
    "            map(t -> t[2], failed_testnames),\n",
    "            map(t -> t[3], failed_testnames), buildcols...], \n",
    "            headers)\n",
    "    )\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
