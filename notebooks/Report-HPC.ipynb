{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC Test Report\n",
    "\n",
    "This is a *proof of concept* for a HPC test results report. It is part of the [JDP](https://rpalethorpe.io.suse.de/jdp/) project. This report is created using Jupyter and Julia, for details see the (highly annotated) general [kernel group report](https://rpalethorpe.io.suse.de/jdp/reports/Report-DataFrames.html).\n",
    "\n",
    "## Index\n",
    "\n",
    "1. [Setup](#Setup)\n",
    "2. [Results](#Results)\n",
    "\n",
    "## Setup\n",
    "\n",
    "First we need to build up our data structures to create the test matrix. There are some stats here which may be useful, but otherwise you can safely skip this part most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitors library source files and recompiles them after most changes\n",
    "import Revise\n",
    "\n",
    "# Run the init script which will setup the JDP project if necessary\n",
    "include(\"../src/init.jl\")\n",
    "\n",
    "# Bring DataFrame's _members_ into our namespace, so we can call them directly\n",
    "using DataFrames\n",
    "\n",
    "# import the markdown string literal/macro\n",
    "import Markdown: @md_str\n",
    "\n",
    "# Import some libraries from the JDP project\n",
    "using JDP.Conf\n",
    "using JDP.Trackers.OpenQA    # Contains functions for dealing with the OpenQA web API\n",
    "using JDP.Trackers.Bugzilla  # Functions for accessing the Bugzilla API(s)\n",
    "using JDP.Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allres = Repository.fetch(OpenQA.TestResult, Vector, \"osd\"; refresh=true, groupid=130)\n",
    "\n",
    "md\"We have **$(length(allres))** results in total\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allhpcres = filter(allres) do res\n",
    "    get(res.suit, 2, nothing) == \"HPC\"\n",
    "end\n",
    "\n",
    "md\"We have **$(length(allhpcres))** HPC test module results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"sle-15-SP1-Installer-DVD\"\n",
    "\n",
    "hpcres = filter(allhpcres) do res\n",
    "    res.product == product\n",
    "end\n",
    "\n",
    "md\"We have **$(length(hpcres))** HPC test results for $product\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builds = map(res -> res.build, hpcres) |> unique\n",
    "builds = map(b -> (parse(Float64, b), b), builds)\n",
    "sort!(builds, by=(b -> b[1]))\n",
    "\n",
    "totalbuilds = length(builds)\n",
    "recentnum = max(0, min(6, totalbuilds - 6))\n",
    "recentbuilds = join(map(b -> \"**$(b[2])**\", builds[end-recentnum+1:end]), \", \", \" and \")\n",
    "\n",
    "md\"We have **$totalbuilds** builds in total. The last $recentnum are $recentbuilds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = vcat([Symbol(\"Job name\"), Symbol(\"Module name\")], \n",
    "    map(b -> Symbol(b[2]), builds[end-recentnum:end]))\n",
    "testnames = map(res -> (res.suit[3], res.name), hpcres) |> unique |> sort\n",
    "\n",
    "buildsres = Dict{String, Dict{Tuple{String, String}, Union{Nothing, OpenQA.TestResult}}}(\n",
    "    build[2] => Dict(name => nothing for name in testnames) for \n",
    "        build in builds[end-recentnum:end]\n",
    ")\n",
    "\n",
    "for res in Iterators.filter(res -> haskey(buildsres, res.build), hpcres)\n",
    "    name = (res.suit[3], res.name)\n",
    "    buildres = buildsres[res.build]\n",
    "    if buildres[name] == nothing || buildres[name].result != \"passed\"\n",
    "        buildres[name] = res\n",
    "    end \n",
    "end\n",
    "\n",
    "failed_testnames = []\n",
    "for name in testnames\n",
    "    boring = true\n",
    "    \n",
    "    for build in keys(buildsres)\n",
    "        res = buildsres[build][name] \n",
    "        if res == nothing || res.result != \"passed\"\n",
    "            boring = false\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if boring\n",
    "        for build in keys(buildsres)\n",
    "            delete!(buildsres[build], name)\n",
    "        end\n",
    "    else\n",
    "        push!(failed_testnames, name)\n",
    "    end\n",
    "end\n",
    "\n",
    "buildcols = [[let res = buildsres[build[2]][name]\n",
    "    res == nothing ? \"none\" : res.result\n",
    "end for name in failed_testnames] for build in builds[end-recentnum:end]]\n",
    "\n",
    "md\"\"\"\n",
    "Ignoring **$(length(testnames) - length(failed_testnames))** of **$(length(testnames))** tests\n",
    "because they only had pass results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Below is a matrix of the HPC results. Test scenarious which always pass have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "withenv(\"LINES\" => 100) do\n",
    "display(DataFrame([map(t -> t[1], failed_testnames), map(t -> t[2], failed_testnames), buildcols...], \n",
    "            headers))\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
