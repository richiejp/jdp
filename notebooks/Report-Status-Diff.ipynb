{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Status Differences\n",
    "\n",
    "This script looks for differences between test results to find interesting changes. When it finds something which may be relevant it can notify any interested parties. This uses the [JDP framework](https://rpalethorpe.io.suse.de/jdp/).\n",
    "\n",
    "First we need to build up our data structures to create the test matrix. There are some stats here which may be useful, but otherwise you can safely skip this part most of the time.\n",
    "\n",
    "## Contents\n",
    "\n",
    "- [Setup](#setup)\n",
    "- Results\n",
    "  + [LTP](#ltp)\n",
    "  + [HPC](#hpc)\n",
    "  + [Networking](#networking)\n",
    "  + [Public Cloud](#publiccloud)\n",
    "  + [Other](#other)\n",
    "  + [File Systems](#fstests)\n",
    "    * [BTRFS](#btrfs)\n",
    "    * [XFS](#xfs)\n",
    "- [Notifications](#notifications)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitors library source files and recompiles them after most changes\n",
    "import Revise\n",
    "\n",
    "# Run the init script which will setup the JDP project if necessary\n",
    "include(\"../src/init.jl\")\n",
    "\n",
    "# Bring DataFrame's _members_ into our namespace, so we can call them directly\n",
    "using DataFrames\n",
    "import DataStructures: SortedDict, SortedSet, SDSemiToken\n",
    "import Dates: Day\n",
    "import TOML\n",
    "\n",
    "# import the markdown string literal/macro\n",
    "import Markdown\n",
    "import Markdown: @md_str, MD\n",
    "\n",
    "# Import some libraries from the JDP project\n",
    "using JDP.Conf\n",
    "using JDP.Trackers.OpenQA    # Contains functions for dealing with the OpenQA web API\n",
    "using JDP.Trackers.Bugzilla  # Functions for accessing the Bugzilla API(s)\n",
    "using JDP.Repository\n",
    "using JDP.Spammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html\"<h2 id='setup'>Setup</h2>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load a large chunk of the results in our database into memory where we can play with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allres = Repository.fetch(OpenQA.TestResult, Vector, \"osd\", OpenQA.RecentOrInterestingJobsDef)\n",
    "\n",
    "md\"We have **$(length(allres))** results in total\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only show results for a single product, which can be set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"sle-12-SP5-Server-DVD\"\n",
    "cloudproduct = \"sle-12-SP5\"\n",
    "\n",
    "prodres = filter(allres) do res\n",
    "    res.product == product\n",
    "end\n",
    "\n",
    "cloudprodres = filter(allres) do res\n",
    "    startswith(res.product, cloudproduct) && (\"Public Cloud\" in res.flags)\n",
    "end\n",
    "\n",
    "md\"\"\"\n",
    "We have $(length(prodres)) test results for $(product) and $(length(cloudprodres)) for $(cloudproduct) cloud\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a 'build matrix', which has one result for each product build. The field subset ordering decides which test result fields are used to decide whether two test results are equal and how they are ordered.\n",
    "\n",
    "The function `OpenQA.describe` is used to return a summary of the result matrix. Otherwise this report would be a little verbose. You can safely remove the describe to see what that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullm = OpenQA.build_matrix(prodres, \n",
    "    OpenQA.FieldSubsetOrdering(:suit, :machine, :name, :flags))\n",
    "OpenQA.describe(fullm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullcm = OpenQA.build_matrix(cloudprodres, \n",
    "    OpenQA.FieldSubsetOrdering(:machine, :suit, :name, :flags))\n",
    "OpenQA.describe(fullcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove older builds and tests only present in those builds for speed and to avoid counting tests which have been permanently disabled in the missing stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = OpenQA.truncate_builds(fullm, 7)\n",
    "OpenQA.describe(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = OpenQA.truncate_builds(fullcm, 7)\n",
    "OpenQA.describe(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions which are used in filtering for each test suite. The functions `OpenQA.filter_builds`, `OpenQA.filter_seqs` and `OpenQA.group_matrix` are fairly generic. Although not as generic as using `DataFrames` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes builds where some percentage of the tests returned no result\n",
    "function filter_bad_builds(mat, tolerance::Float64)\n",
    "    tcount = length(mat.seqs) # seqs is short for test sequences\n",
    "    \n",
    "    OpenQA.filter_builds(mat) do builds\n",
    "        nons = 0\n",
    "        for testres in builds\n",
    "            if testres == nothing\n",
    "                nons += 1\n",
    "            end\n",
    "        end\n",
    "        nons / tcount < tolerance\n",
    "    end\n",
    "end\n",
    "\n",
    "# removes tests which returned the same result for all builds\n",
    "function filter_consistant_tests(mat)\n",
    "    OpenQA.filter_seqs(mat) do ex, seq # ex is short for exemplar test\n",
    "        ftest = first(seq)\n",
    "        fres = ftest == nothing ? \"none\" : ftest.result\n",
    "        !all(seq) do test\n",
    "            res = test == nothing ? \"none\" : test.result\n",
    "            res == fres\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function group_by_machine(mat)\n",
    "    # Note that tests are implicitly grouped by the result status sequence\n",
    "    # as well the function passed here\n",
    "    OpenQA.group_matrix(mat) do test1, test2\n",
    "        test1.suit == test2.suit\n",
    "    end\n",
    "end\n",
    "\n",
    "# Usually the results would be limited to approximately your display size\n",
    "ENV[\"LINES\"] = 500\n",
    "\n",
    "function filter_and_group(fn, mat, tolerance)\n",
    "    mat = OpenQA.filter_seqs(fn, mat)\n",
    "    display(md\"After test filter: $(OpenQA.describe(mat))\")\n",
    "    mat = filter_bad_builds(mat, tolerance)\n",
    "    display(md\"After bad build filter: $(OpenQA.describe(mat))\")\n",
    "    mat = filter_consistant_tests(mat)\n",
    "    display(md\"After consistant test filter: $(OpenQA.describe(mat))\")\n",
    "    group_by_machine(mat)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The results of a number of different test suites or environments follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html\"<h3 id='ltp'>LTP</h3>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltpmg = filter_and_group(m, 0.25) do ex, seq\n",
    "    ex.suit[1] == \"LTP\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html\"<h3 id='hpc'>HPC</h3>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpcmg = filter_and_group(m, 0.25) do ex, seq\n",
    "    length(ex.suit) > 1 && ex.suit[1:2] == [\"OpenQA\", \"HPC\"]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html\"<h3 id='networking'>Networking</h3>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netmg = filter_and_group(m, 0.25) do ex, seq\n",
    "    occursin(\"wicked\", ex.job.name)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "html\"<h3 id='publiccloud'>Public Cloud</h3>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmg = filter_and_group(cm, 0.25) do ex, seq\n",
    "    true\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "html\"<h3 id='other'>Other</h3>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the tests listed here are simply OpenQA helper modules or tests which have not been properly categorised yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "othmg = filter_and_group(m, 0.25) do ex, seq\n",
    "    suit = ex.suit[1]\n",
    "    \n",
    "    suit ≠ \"LTP\" && suit ≠ \"fstests\" && \n",
    "    !(\"Public Cloud\" in ex.flags) && \n",
    "    get(ex.suit, 2, nothing) ≠ \"HPC\" &&\n",
    "    !occursin(\"wicked\", ex.job.name)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html\"<h3 id='fstests'>File Systems</h3><h4 id='btrfs'>BTRFS</h4>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btrfsmg = filter_and_group(m, 0.15) do ex, seq\n",
    "    ex.suit[1] == \"fstests\" && ex.suit[2] == \"btrfs\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html\"<h4 id='xfs'>XFS</h4>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfsmg = filter_and_group(m, 0.15) do ex, seq\n",
    "    ex.suit[1] == \"fstests\" && ex.suit[2] == \"xfs\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html\"<h2 id='notifications'>Notifications</h2>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we notify interested persons of the changes in test results. To limit the amount of noise, each test can only be included in a notification to the specified set of users once a month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function maybe_notify(gm, report_id, notifyprefs)\n",
    "    mentions = Set()\n",
    "    changed_tests = 0\n",
    "    if isempty(gm.m.builds) \n",
    "        return changed_tests\n",
    "    end\n",
    "    build = first(gm.m.builds)\n",
    "    \n",
    "    # Notifications are not effective if there are too many of them. Also setting the\n",
    "    # notified flags for each users-test pair can be expensive.\n",
    "    if length(gm.groups) > 100\n",
    "        @warn \"No notifications will be sent for $report_id due to the excessive number of changes\"\n",
    "        return 0\n",
    "    end\n",
    "\n",
    "    for g in gm.groups\n",
    "        test = first(g.tests)\n",
    "        test_name = join(test.suit, \":\") * \":$(test.name)\"\n",
    "        test_id = \"$test_name@$(test.arch)[\" * join(test.flags, \",\") * \"]\"\n",
    "        users = vcat((users for (pattern, users) in notifyprefs if occursin(pattern, test_id))...)\n",
    "        users_key = join(users, \"&\")\n",
    "        flag_key = \"diff-notified-$test_id$users_key\"\n",
    "        latest = if haskey(g.seq.builds, build)\n",
    "            g.seq.builds[build]\n",
    "        else\n",
    "            nothing\n",
    "        end\n",
    "                            \n",
    "        oldres = Repository.get_temp_flag(flag_key)\n",
    "        newres = latest ≠ nothing ? latest.result : \"none\"\n",
    "        @debug test_id repr(oldres) newres\n",
    "        if oldres ≠ newres\n",
    "            changed_tests += 1\n",
    "            push!(mentions, users...)\n",
    "            Repository.set_temp_flag(flag_key, newres, Day(7))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if changed_tests > 0\n",
    "        io = IOBuffer()\n",
    "        print(io, \"\"\"\n",
    "At least $changed_tests tests appear to have changed status recently in the $report_id category.\\n\n",
    "See the [Status Difference Report](https://rpalethorpe.io.suse.de/jdp/reports/Report-Status-Diff.html#$report_id) for details\"\"\")\n",
    "\n",
    "        Spammer.post_message(Spammer.Message(String(take!(io)), collect(mentions)))\n",
    "    end\n",
    "    \n",
    "    changed_tests\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The targets of the notifications are taken from the OpenQA job group descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testprefs = OpenQA.load_notify_preferences(\"osd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = maybe_notify(ltpmg, \"ltp\", testprefs)\n",
    "md\"Sent **$changes** change notifications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = maybe_notify(hpcmg, \"hpc\", testprefs)\n",
    "md\"Sent **$changes** change notifications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = maybe_notify(netmg, \"network\", testprefs)\n",
    "md\"Sent **$changes** change notifications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = maybe_notify(cmg, \"publiccloud\", testprefs)\n",
    "md\"Sent **$changes** change notifications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = maybe_notify(othmg, \"other\", testprefs)\n",
    "md\"Sent **$changes** change notifications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = maybe_notify(btrfsmg, \"btrfs\", testprefs)\n",
    "md\"Sent **$changes** change notifications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = maybe_notify(xfsmg, \"xfs\", testprefs)\n",
    "md\"Sent **$changes** change notifications\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
