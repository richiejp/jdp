{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results Review Report\n",
    "\n",
    "This is supposed to help create the milestone report for now and eventuall generate it automatically. It is an interactive Jupyter document (or a static view of such a document) containing Julia code segments and their output. It is part of the JDP project which aims to create an *easily accessible* system for exploring test results and automating *arbitrary* workflows. Notebooks such as these are intended to provide an easy starting point for engineers and other technical users to create their own reports, possibly just by tweaking the existing ones.\n",
    "\n",
    "Eventually this workbook should be easily installable locally or accessible from some remote location with zero knowledge of Julia or Jupyter. However right now it is not, but you can still try by visiting: https://github.com/richiejp/jdp.\n",
    "\n",
    "Obviously you can also access the library from a REPL or use it in a traditional script or application, but Jupyter provides a nice, persistent, graphical environment. I won't discuss how to use Jupyter in this notebook (just click on help at the top), but will heavily annotate the code.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First we need to load the JDP library which does the heavy lifting; importing and transforming the test result data from OpenQA into something useable. Note that this assumes you started this notebook by running `julia src/notebook.jl`.\n",
    "\n",
    "> NOTE: It is required that you run this cell before the code cells following it. However not all of the cells need to be executed in order.\n",
    "\n",
    "You may see a bunch of horrible angry red text when running this. Unfortunately this could either be info messages or error messages from the logging system, Jupyter treats both to a red background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating JDP package at /home/rich/julia/jdp/\n",
      "Installing project deps if necessary...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[2K\u001b[?25h[1mFetching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %\u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [============>                            ]  27.6 %                 ]  55.0 %\u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [========================>                ]  59.4 %>     ]  86.8 %"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/rich/.julia/compiled/v1.0/DataFrames/AR9oZ.ji for DataFrames [a93c6f00-e57d-5684-b7b6-d8193f3e46c0]\n",
      "└ @ Base loading.jl:1184\n",
      "┌ Info: Recompiling stale cache file /home/rich/.julia/compiled/v1.0/JDP/uw0DL.ji for JDP [6d7e372e-c0bd-11e8-2b07-216b8359d694]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    }
   ],
   "source": [
    "# Run the install script which will setup the JDP project if necessary\n",
    "include(\"install.jl\")\n",
    "\n",
    "# Bring DataFrame's _members_ into our namespace, so we can call them directly\n",
    "using DataFrames\n",
    "\n",
    "# Import some libraries from the JDP project\n",
    "using JDP.OpenQA    # Contains functions for dealing with the OpenQA web API\n",
    "using JDP.TableDB   # Functions for accessing test data in table like formats (currently DataFrames)\n",
    "using JDP.Bugzilla  # Functions for accessing the Bugzilla API(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set some variables which are used in later code cells. Cache type can be set to `:json` or `:binary` (which are [symbols](https://docs.julialang.org/en/v1/manual/metaprogramming/#Symbols-1)). We always save data as JSON first, but afterwards it can be copied into a binary format as well to increase loading speed.\n",
    "\n",
    "> NOTE: Julia has a _very_ strong type system, but we can still assign variables like a dynamic language. For library code it is generally a good idea to explicitly state what types you are expecting, but in Notebook code we can just let the compiler guess the type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":json"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datadir = \"/home/rich/qa/data/osd\" # The cache dir for the OpenQA test result data\n",
    "cache_type = :json # Set to :json to use the raw JSON data from OpenQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we may download some new results for a given build or builds to our local cache. This usually takes a long time, hence why there is a local cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over an array of strings... yeah I bet you really needed to be told that didn't you?\n",
    "for jid in [\"426\", \"429\", \"431\", \"432\", \"435\"]\n",
    "    # Get some job results from the openqa.suse.de (osd) OpenQA instance.\n",
    "    # Optional arguments (after the ';') like 'build' and 'groupid' are passed to the OpenQA API\n",
    "    OpenQA.save_job_results_json(OpenQA.osd, datadir; build=\"0$jid\", groupid=\"155\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data into memory, this can also take a while. If we don't have any new data to load we can use the binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Loaded 500985 results\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the variables are defined in the global scope\n",
    "json = nothing\n",
    "df = nothing\n",
    "\n",
    "if cache_type == :binary\n",
    "    # The raw data from OpenQA is absurdly huge, so to save on start up time, we can use a binary format\n",
    "    df = TableDB.load_module_results(joinpath(datadir, \"cache.jld2\"))\n",
    "else\n",
    "    json = OpenQA.load_job_results_json(datadir) # This is the raw OpenQA data in the form of a Dict\n",
    "    df = TableDB.get_module_results(json)        # This is a more refined form of the data as a DataFrame\n",
    "end\n",
    "\n",
    "\"Loaded $(nrow(df)) results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have some new data then we can update the binary cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TableDB.save_module_results(joinpath(datadir, \"cache.jld2\"), df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `describe` from the DataFrames package gives us some stats and information about the structure of the loaded data. For the raw json value we can just use `summary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"10929-element Array{Dict{String,Any},1}\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>nunique</th><th>min</th><th>max</th><th>eltype</th></tr></thead><tbody><tr><th>1</th><td>build</td><td>48</td><td>0250</td><td>0419</td><td>String</td></tr><tr><th>2</th><td>name</td><td>5810</td><td>1_autotest</td><td>zram03</td><td>String</td></tr><tr><th>3</th><td>result</td><td>5</td><td>canceled</td><td>softfailed</td><td>String</td></tr><tr><th>4</th><td>arch</td><td>4</td><td>aarch64</td><td>x86_64</td><td>String</td></tr><tr><th>5</th><td>suit</td><td>71</td><td>(\"LTP\", \"can\")</td><td>(\"fstests\", \"xfs\")</td><td>Tuple{String,Union{Missing, String}}</td></tr><tr><th>6</th><td>bugrefs</td><td>161</td><td>[]</td><td>[\"t#2094962\"]</td><td>Array{SubString,N} where N</td></tr></tbody></table>"
      ],
      "text/plain": [
       "6×5 DataFrame. Omitted printing of 1 columns\n",
       "│ Row │ variable │ nunique │ min            │ max                │\n",
       "│     │ \u001b[90mSymbol\u001b[39m   │ \u001b[90mInt64\u001b[39m   │ \u001b[90mAny\u001b[39m            │ \u001b[90mAny\u001b[39m                │\n",
       "├─────┼──────────┼─────────┼────────────────┼────────────────────┤\n",
       "│ 1   │ build    │ 48      │ 0250           │ 0419               │\n",
       "│ 2   │ name     │ 5810    │ 1_autotest     │ zram03             │\n",
       "│ 3   │ result   │ 5       │ canceled       │ softfailed         │\n",
       "│ 4   │ arch     │ 4       │ aarch64        │ x86_64             │\n",
       "│ 5   │ suit     │ 71      │ (\"LTP\", \"can\") │ (\"fstests\", \"xfs\") │\n",
       "│ 6   │ bugrefs  │ 161     │ []             │ [\"t#2094962\"]      │"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(df, stats = [:nunique, :min, :max, :eltype])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the pretty table! We can also display graphs which could be even more delightful. Unfortunately it is slightly less pretty if you are viewing this as a static page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed tests for build\n",
    "\n",
    "Let's look at what tests failed for a given build. First we need to filter out passed test results and results from other builds. Then we can group the results by test name and suit, amalgamating some of the columns to make the table easier to view. Filter is fairly simple, but the grouping is a bit more complex and there is a bit of Julia magic, see [Split-Apply-Combine](http://juliadata.github.io/DataFrames.jl/stable/man/split_apply_combine.html) for help.\n",
    "\n",
    "> NOTE: Packages such as QUERY.jl allow one to use an SQL like syntax which is probably a lot easier to understand for most people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"73 tests failed this build\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build = \"0435\"\n",
    "\n",
    "# The syntax \"var -> expr\" is an anonymous function, strings starting with 'r' are regexs.\n",
    "# In Julia you don't need to write 'return' (unless you want to return early), most \n",
    "# statements return whatever the value of the final expression is\n",
    "fails = filter(r -> r.build == build && occursin(r\"failed\", r[:result]), df)\n",
    "\n",
    "# group by name then apply the function defined by `do r ...` to each group\n",
    "# Putting `do r` after `by` is like writing `by(r -> ...`. i.e. `do r` defines a function\n",
    "# and passes it as the first argument to `by`.\n",
    "fails_by_name = by(fails, [:name, :suit]) do r\n",
    "    # 'by' first groups the results by name and suit then passes each group to us in the variable 'r'\n",
    "    # we then use 'r' to produce a new DataFrame containing a single row. We return the new DataFrames \n",
    "    # and `by` then combines them... at least I think that is what happpens.\n",
    "    DataFrame(\n",
    "        # We have to write Tuple otherwise DataFrame creates a multi-row result (because r.result is an array)\n",
    "        result = Tuple(unique(r.result)),\n",
    "        arch = Tuple(unique(r.arch)),\n",
    "        # Three dots `...` 'splats' an array (or tuple) into multiple function arguments \n",
    "        # and `vcat` concatenates it's arguments together\n",
    "        bugrefs = Tuple(unique(vcat(r.bugrefs...)))\n",
    "        # also, and don't panic if this is a little more difficult to understand, \n",
    "        # 'unique' removes duplicate elements from a collection\n",
    "    )\n",
    "end\n",
    "\n",
    "\"$(nrow(fails_by_name)) tests failed this build\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably have too many failures to display in Jupyter, so let's just try displaying failures for a subset of tests. We can focus on a particular test suit and remove tests which already appear to be tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>name</th><th>suit</th><th>result</th><th>arch</th><th>bugrefs</th></tr><tr><th></th><th>String</th><th>Tuple…</th><th>Tuple…</th><th>Tuple…</th><th>Tuple…</th></tr></thead><tbody><tr><th>1</th><td>epoll_wait02</td><td>(\"LTP\", \"syscalls\")</td><td>(\"failed\",)</td><td>(\"s390x\",)</td><td>()</td></tr><tr><th>2</th><td>statx05</td><td>(\"LTP\", \"syscalls\")</td><td>(\"failed\",)</td><td>(\"s390x\",)</td><td>()</td></tr></tbody></table>"
      ],
      "text/plain": [
       "2×5 DataFrame. Omitted printing of 1 columns\n",
       "│ Row │ name         │ suit                │ result      │ arch       │\n",
       "│     │ \u001b[90mString\u001b[39m       │ \u001b[90mTuple…\u001b[39m              │ \u001b[90mTuple…\u001b[39m      │ \u001b[90mTuple…\u001b[39m     │\n",
       "├─────┼──────────────┼─────────────────────┼─────────────┼────────────┤\n",
       "│ 1   │ epoll_wait02 │ (\"LTP\", \"syscalls\") │ (\"failed\",) │ (\"s390x\",) │\n",
       "│ 2   │ statx05      │ (\"LTP\", \"syscalls\") │ (\"failed\",) │ (\"s390x\",) │"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_bugrefs = filter(fails_by_name) do r\n",
    "    length(r.bugrefs) < 1 && # Remove tests which already have bug refs\n",
    "    r.suit[1] == \"LTP\" &&    # Only include LTP results\n",
    "    r.name != \"boot_ltp\" &&  # Don't include boot_ltp and shutdown_ltp modules\n",
    "    r.name != \"shutdown_ltp\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>name</th><th>suit</th><th>result</th><th>arch</th><th>bugrefs</th></tr><tr><th></th><th>String</th><th>Tuple…</th><th>Tuple…</th><th>Tuple…</th><th>Tuple…</th></tr></thead><tbody><tr><th>1</th><td>xfs-083</td><td>(\"fstests\", \"xfs\")</td><td>(\"failed\",)</td><td>(\"aarch64\",)</td><td>()</td></tr><tr><th>2</th><td>xfs-491</td><td>(\"fstests\", \"xfs\")</td><td>(\"failed\",)</td><td>(\"aarch64\", \"ppc64le\", \"x86_64\")</td><td>()</td></tr><tr><th>3</th><td>xfs-492</td><td>(\"fstests\", \"xfs\")</td><td>(\"failed\",)</td><td>(\"aarch64\", \"ppc64le\", \"x86_64\")</td><td>()</td></tr><tr><th>4</th><td>xfs-493</td><td>(\"fstests\", \"xfs\")</td><td>(\"failed\",)</td><td>(\"aarch64\", \"ppc64le\", \"x86_64\")</td><td>()</td></tr><tr><th>5</th><td>generic-445</td><td>(\"fstests\", \"xfs\")</td><td>(\"failed\",)</td><td>(\"ppc64le\",)</td><td>()</td></tr><tr><th>6</th><td>xfs-173</td><td>(\"fstests\", \"xfs\")</td><td>(\"failed\",)</td><td>(\"ppc64le\",)</td><td>()</td></tr></tbody></table>"
      ],
      "text/plain": [
       "6×5 DataFrame. Omitted printing of 2 columns\n",
       "│ Row │ name        │ suit               │ result      │\n",
       "│     │ \u001b[90mString\u001b[39m      │ \u001b[90mTuple…\u001b[39m             │ \u001b[90mTuple…\u001b[39m      │\n",
       "├─────┼─────────────┼────────────────────┼─────────────┤\n",
       "│ 1   │ xfs-083     │ (\"fstests\", \"xfs\") │ (\"failed\",) │\n",
       "│ 2   │ xfs-491     │ (\"fstests\", \"xfs\") │ (\"failed\",) │\n",
       "│ 3   │ xfs-492     │ (\"fstests\", \"xfs\") │ (\"failed\",) │\n",
       "│ 4   │ xfs-493     │ (\"fstests\", \"xfs\") │ (\"failed\",) │\n",
       "│ 5   │ generic-445 │ (\"fstests\", \"xfs\") │ (\"failed\",) │\n",
       "│ 6   │ xfs-173     │ (\"fstests\", \"xfs\") │ (\"failed\",) │"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_bugrefs = filter(fails_by_name) do r\n",
    "    length(r.bugrefs) < 1 &&\n",
    "    r.suit == (\"fstests\", \"xfs\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find if any of these tests had bug refs in past builds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>name</th><th>suit</th><th>bugrefs</th></tr><tr><th></th><th>String</th><th>Tuple…</th><th>Tuple…</th></tr></thead><tbody><tr><th>1</th><td>generic-445</td><td>(\"fstests\", \"btrfs\")</td><td>(\"bsc#1103543\",)</td></tr><tr><th>2</th><td>xfs-083</td><td>(\"fstests\", \"xfs\")</td><td>(\"bsc#1105017\",)</td></tr><tr><th>3</th><td>generic-445</td><td>(\"fstests\", \"xfs\")</td><td>(\"bsc#1073390\", \"bsc#1105025\")</td></tr><tr><th>4</th><td>xfs-173</td><td>(\"fstests\", \"xfs\")</td><td>(\"bsc#1073390\", \"bsc#1105025\")</td></tr></tbody></table>"
      ],
      "text/plain": [
       "4×3 DataFrame\n",
       "│ Row │ name        │ suit                 │ bugrefs                        │\n",
       "│     │ \u001b[90mString\u001b[39m      │ \u001b[90mTuple…\u001b[39m               │ \u001b[90mTuple…\u001b[39m                         │\n",
       "├─────┼─────────────┼──────────────────────┼────────────────────────────────┤\n",
       "│ 1   │ generic-445 │ (\"fstests\", \"btrfs\") │ (\"bsc#1103543\",)               │\n",
       "│ 2   │ xfs-083     │ (\"fstests\", \"xfs\")   │ (\"bsc#1105017\",)               │\n",
       "│ 3   │ generic-445 │ (\"fstests\", \"xfs\")   │ (\"bsc#1073390\", \"bsc#1105025\") │\n",
       "│ 4   │ xfs-173     │ (\"fstests\", \"xfs\")   │ (\"bsc#1073390\", \"bsc#1105025\") │"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_index = Set(missing_bugrefs.name) # Convert the name column into a hash set\n",
    "\n",
    "past_results = by(filter(r -> r.name in names_index && length(r.bugrefs) > 0, df), [:name, :suit]) do r\n",
    "    DataFrame(bugrefs = Tuple(\n",
    "        # Remove OpenQA's self references with !startswith\n",
    "        filter(br -> !startswith(br, \"t#\"), unique(vcat(r.bugrefs...)))\n",
    "    ))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may still find that there are still too many results to view here. It is left as an excercise to the reader to filter out even more (you may just want to blacklist tests like `boot_ltp` and `partition` which create a lot of noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to judge from looking at a bug reference what it is about and whether it is relevant to a particular test failure. The `BugRefs.ipynb` notebook provides indepth tools for dealing with trackers and bug references, but we can also display a summary of the bug info here. First we need to login to Bugzilla, a user name and password prompt are displayed if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Name: rpalethorpe\n",
      "Password: ········\n"
     ]
    }
   ],
   "source": [
    "bsc_ses = Bugzilla.login(\"bsc\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the amazing ability to produce Markdown programatically and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**P3 - Medium**(*Normal*) NEW: fstests with xfs on generic/042 fails with difference on golden output on 4.12.14-4.7-default\n",
       "\n",
       "**P2 - High**(*Normal*) NEW: xfstests generic/486 fail in xfs\n",
       "\n",
       "**P2 - High**(*Normal*) NEW: xfstests xfs/013 fails in ppc64le occasionally\n",
       "\n",
       "**P2 - High**(*Normal*) RESOLVED: xftests generic/502 fails for btrfs\n"
      ],
      "text/plain": [
       "  \u001b[1mP3 - Medium\u001b[22m(\u001b[4mNormal\u001b[24m) NEW: fstests with xfs on generic/042 fails with\n",
       "  difference on golden output on 4.12.14-4.7-default\n",
       "\n",
       "  \u001b[1mP2 - High\u001b[22m(\u001b[4mNormal\u001b[24m) NEW: xfstests generic/486 fail in xfs\n",
       "\n",
       "  \u001b[1mP2 - High\u001b[22m(\u001b[4mNormal\u001b[24m) NEW: xfstests xfs/013 fails in ppc64le occasionally\n",
       "\n",
       "  \u001b[1mP2 - High\u001b[22m(\u001b[4mNormal\u001b[24m) RESOLVED: xftests generic/502 fails for btrfs"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Markdown: MD\n",
    "\n",
    "refs = foldl(past_results.bugrefs; init=[]) do acc, brefs\n",
    "    vcat([brefs...], acc)\n",
    "end\n",
    "# The pipe operator '|>' pipes. This is the same as writting unique(filter(...)), but has the advantage\n",
    "# of confusing some people who have not see it before\n",
    "refs = filter(ref -> startswith(ref, \"bsc\"), refs) |> unique\n",
    "bugs = map(ref -> Bugzilla.get_bug(bsc_ses, parse(Int, ref[5:end])), refs)\n",
    "map(Bugzilla.to_md, bugs) |> MD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's get the completely tagless tests on their own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>name</th><th>suit</th><th>arch</th><th>frequency</th></tr><tr><th></th><th>String</th><th>Tuple…</th><th>Tuple…</th><th>Int64</th></tr></thead><tbody><tr><th>1</th><td>generic-445</td><td>(\"fstests\", \"xfs\")</td><td>(\"ppc64le\",)</td><td>6</td></tr><tr><th>2</th><td>xfs-173</td><td>(\"fstests\", \"xfs\")</td><td>(\"ppc64le\",)</td><td>2</td></tr><tr><th>3</th><td>xfs-083</td><td>(\"fstests\", \"xfs\")</td><td>(\"aarch64\", \"x86_64\")</td><td>4</td></tr><tr><th>4</th><td>xfs-491</td><td>(\"fstests\", \"xfs\")</td><td>(\"ppc64le\", \"x86_64\", \"aarch64\")</td><td>5</td></tr><tr><th>5</th><td>xfs-492</td><td>(\"fstests\", \"xfs\")</td><td>(\"ppc64le\", \"x86_64\", \"aarch64\")</td><td>5</td></tr><tr><th>6</th><td>xfs-493</td><td>(\"fstests\", \"xfs\")</td><td>(\"ppc64le\", \"x86_64\", \"aarch64\")</td><td>5</td></tr></tbody></table>"
      ],
      "text/plain": [
       "6×4 DataFrame. Omitted printing of 1 columns\n",
       "│ Row │ name        │ suit               │ arch                             │\n",
       "│     │ \u001b[90mString\u001b[39m      │ \u001b[90mTuple…\u001b[39m             │ \u001b[90mTuple…\u001b[39m                           │\n",
       "├─────┼─────────────┼────────────────────┼──────────────────────────────────┤\n",
       "│ 1   │ generic-445 │ (\"fstests\", \"xfs\") │ (\"ppc64le\",)                     │\n",
       "│ 2   │ xfs-173     │ (\"fstests\", \"xfs\") │ (\"ppc64le\",)                     │\n",
       "│ 3   │ xfs-083     │ (\"fstests\", \"xfs\") │ (\"aarch64\", \"x86_64\")            │\n",
       "│ 4   │ xfs-491     │ (\"fstests\", \"xfs\") │ (\"ppc64le\", \"x86_64\", \"aarch64\") │\n",
       "│ 5   │ xfs-492     │ (\"fstests\", \"xfs\") │ (\"ppc64le\", \"x86_64\", \"aarch64\") │\n",
       "│ 6   │ xfs-493     │ (\"fstests\", \"xfs\") │ (\"ppc64le\", \"x86_64\", \"aarch64\") │"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_index = Set(missing_bugrefs.name)\n",
    "by(filter(r -> r.name in names_index && length(r.bugrefs) < 1 && occursin(r\"failed\", r.result), df), [:name, :suit]) do r\n",
    "    DataFrame(\n",
    "        arch = Tuple(unique(r.arch)),\n",
    "        frequency = length(unique(r.build))\n",
    "    )\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
